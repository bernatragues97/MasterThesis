{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Python scraper for tripadvisor**\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USED LIBRARIES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#---llibreries afegides\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import json\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING THE WEB DRIVER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening a new automated chrome tab with selenium webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to set some parameters on the driver so that it can give me the requests it makes to the server.\n",
    "desired_capabilities = DesiredCapabilities.CHROME\n",
    "desired_capabilities[\"goog:loggingPrefs\"] = {\"performance\": \"ALL\"}\n",
    "  \n",
    "driver = webdriver.Chrome('/Users/user/Desktop/JupyLab/webdriver/chromedriver110', desired_capabilities=desired_capabilities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBTAINING A LIST OF RESTAURANTS IN AARHUS THROUGH SCRAPING TECHNIQUES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to opening tripadvisor website. First thing we find is a pop up asking to accept privacy and cookies. We make the robot click accept with find element (we inspect the element \"Acepto\" in this case and we click by the xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.tripadvisor.com/Restaurants-g189530-zfn20484093-Aarhus_East_Jutland_Jutland.html')\n",
    "time.sleep(10)\n",
    "try:\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]').click()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of the restaurants that can be found in Aarhus with name as well as the varying elements that can be collected from the webpage link (href), i.e. https://www.tripadvisor.com/Restaurant_Review-g189530-d4505984-Reviews-Ispirazione_Restaurant_Vinbar_Kaffebar_Butik-Aarhus_East_Jutland_Jutland.html. We need them because when scrapping each restaurant reviews we will use them to manipulate the link and go restaurant by restaurant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_list = [] #Create a list to gather all the elements\n",
    "def get_list(soup): #Create a function to list all the restaurants (and the additional info we gather)\n",
    "    \n",
    "    #Loop to gather all the restaurants from the list (In this case page 0: div[data-test*=\"_list_item)\n",
    "    for div in soup.select('div[data-test*=\"_list_item\"]', href = True):\n",
    "        \n",
    "        #We start by gathering data from the link \"href\" that we open for each restaurant. In the link_info \n",
    "        #dictionary we add the rest_name and num_opinions and additioanlly all the elements we are collecting \n",
    "        #from the link\n",
    "        try: \n",
    "             rest_name = div.find(class_='RfBGI').text.split('. ')[1]\n",
    "             geo_locationId = div.find(class_='Lwqic Cj b').get('href').split('-')\n",
    "        except: pass\n",
    "        try:\n",
    "             link_info = {\n",
    "             \"rest_name\": rest_name,\n",
    "             #'num_opinions': div.find(class_ = 'IiChw').text.split(' ')[0], WE NEED TO DEAL WITH NON-REVIEWED RESTS\n",
    "             'geo': geo_locationId[1],\n",
    "             'locationId': geo_locationId[2],\n",
    "             'restaurant_name': geo_locationId[4],\n",
    "             'location': geo_locationId[5]}\n",
    "              \n",
    "             restaurant_list.append(link_info) #Add dictionary elements to the list\n",
    "        except: pass\n",
    "    return restaurant_list\n",
    "\n",
    "# GET RESTAURANT LIST, PARSE HTML FROM PAGE SOURCE FOR PAGE 1\n",
    "#Create element that we introduce in the function to create a list: Parsed html\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser') \n",
    "restaurant_list = get_list(soup) #Run the function\n",
    "\n",
    "\n",
    "# GATHER ALL THE RESTAURANTS FROM PAGE 2 TO PAGE N\n",
    "\n",
    "#Start by clicking next to get to page 2 (we can not put this part in the while loop because the XPATH for the page 1\n",
    "# button \"siguiente\" is different than for the other pages)\n",
    "try: driver.find_element(By.XPATH, '//*[@id=\"EATERY_LIST_CONTENTS\"]/div[3]/div/a').click()\n",
    "except: driver.find_element(By.XPATH, '//*[@id=\"EATERY_LIST_CONTENTS\"]/div[2]/div/a').click()\n",
    "\n",
    "#Loop to go through all the pages gathering all restaurant information described above)\n",
    "while soup.find(class_= 'nav next rndBtn ui_button primary taLnk'):\n",
    "     soup = BeautifulSoup(driver.page_source, 'html.parser') \n",
    "     time.sleep(4)\n",
    "     restaurant_list = restaurant_list = get_list(soup)\n",
    "     try: #We need a try here because for the last page it won't find the button XPATH and we don't \n",
    "          #want to get an exception\n",
    "          driver.find_element(By.XPATH, '//*[@id=\"EATERY_LIST_CONTENTS\"]/div[2]/div/a[2]').click() \n",
    "          time.sleep(4) \n",
    "     except:\n",
    "          break\n",
    "\n",
    "'''\n",
    "print(len(restaurant_list)) #Print number of restaurants in the list\n",
    "for i in restaurant_list: #Print restaurants names\n",
    "    print(i['rest_name']) \n",
    "print(restaurant_list)\n",
    "'''\n",
    "#Save the list as a df    \n",
    "restaurant_list_df = pd.DataFrame (restaurant_list)\n",
    "#restaurant_list_df.to_csv(\"restaurant_list_df\", index = False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the restaurants list in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_list_df.to_csv(\"restaurant_list_df_aarhus\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBTAINING RESTAURANTS LIST FROM THE CSV FILE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the df list saved to avoid having to scrape again all the restaurants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_list_df = pd.read_csv(\"restaurant_list_df_aarhus\", index_col=0, encoding = 'unicode_escape')\n",
    "restaurant_list = restaurant_list_df.to_dict(\"records\")\n",
    "#restaurant_list = restaurant_list[0:5]\n",
    "print(restaurant_list)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBTAINING REVIEWS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that gathers all the reviews (and all its inerent relevant information) for all the restaurants in the previously generated list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(link, driver, page, df):\n",
    "\n",
    "    driver.get(link) #Gets in the first restaurant webpage\n",
    "    time.sleep(random.randint(1,3))\n",
    "    \n",
    "    #In case page for the reviews = 0, then a popup might appear and we make the robot to close it\n",
    "    if page == '0':\n",
    "        driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]').click()\n",
    "        time.sleep(1)\n",
    "\n",
    "    try: \n",
    "        all_languages = driver.find_element(By.XPATH, '//*[@id=\"taplc_detail_filters_rr_resp_0\"]/div/div[1]/div/div[2]/div[4]/div/div[2]/div[1]/div[1]/label')\n",
    "        #time.sleep(1) #Click to all languages, to get reviews in all languages avaiblable\n",
    "        all_languages.click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser') #Create element with html parsed that allows us to extract data from it\n",
    "  \n",
    "    local_name = soup.title.text.split(',')[0] #We don't need to put the name of the restaurant in the reviews loop\n",
    "    #because is always part of the same page. It's in the tab of the website (that's the title)\n",
    "\n",
    "    for div in soup.findAll('div', attrs={'class':'reviewSelector'}): #Loop for the reviews list\n",
    "        \n",
    "        #From the reviews we need: \n",
    "        \n",
    "        #TITLE OF THE REVIEW\n",
    "        title = div.find(class_ = 'noQuotes').text\n",
    "        \n",
    "        #DATE OF THE REVIEW\n",
    "        review_date = div.find(class_ = 'ratingDate')['title']\n",
    "        \n",
    "        #ID OF THE REVIEW\n",
    "        review_id = div.get('id')\n",
    "\n",
    "        #MEMBER NAME + MEMBER ID\n",
    "        try:\n",
    "            member_name = div.find(class_='info_text pointer_cursor').text\n",
    "            member_id_all = div.find(class_= \"memberOverlayLink clickable\")[\"id\"]\n",
    "            member_id = re.split(r'[-_]', member_id_all)[1]\n",
    "            \n",
    "        except:\n",
    "            member_name = 'unknown'\n",
    "            member_id = 'unknown'\n",
    "        \n",
    "        #MEMBER NAME (IN CASE THE REVIEW COMES FROM EL TENEDOR APP)\n",
    "        try:\n",
    "            member_name = div.find(class_='info_text ').text\n",
    "            member_id = member_name\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #DAY OF THE VISIT\n",
    "        try:\n",
    "            visit_day = div.find(class_='prw_rup prw_reviews_stay_date_hsx').text.split(': ')[1]\n",
    "        except:\n",
    "            visit_day = None\n",
    "\n",
    "        #RATING\n",
    "        rating = div.select_one('[class*=\"ui_bubble_rating bubble\"]')['class'][1].split('_')[1][0]\n",
    "        \n",
    "        #TEXT REVIEW\n",
    "        text_review = div.find(class_ = 'entry').text.replace('Más','').replace('...',' ').replace('\\n',' ')\n",
    "        \n",
    "        #REVIEW DONE VIA MOBILE OR NOT\n",
    "        via_mobile = div.find('span', {'class': 'ui_icon mobile-phone'})\n",
    "        if via_mobile:\n",
    "            via_mobile = True\n",
    "        else:\n",
    "            via_mobile = None\n",
    "\n",
    "        dict_aux = {\n",
    "            'restaurant': [local_name],\n",
    "            'review_date': review_date,\n",
    "            'visit_day': visit_day,\n",
    "            'review_id': review_id,\n",
    "            'member_id': member_id,\n",
    "            'member_name': member_name,\n",
    "            'rating': rating,\n",
    "            'title_review': title,\n",
    "            'text_review': text_review,\n",
    "            'via_mobile': via_mobile,\n",
    "            'page': page\n",
    "        }\n",
    "\n",
    "        df_aux = pd.DataFrame(dict_aux) #Names of the columns\n",
    "        df = pd.concat([df, df_aux], axis=0) #Giving names/labels to the columns of our dataframe\n",
    "\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True) #To enumerate the dataframe rows\n",
    "\n",
    "\n",
    "    return df, soup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a loop with the function that gathers all the reviews for each restaurant. We need the loop to access each restaurant using their link and the changing parts that we collected in the restaurants list part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame() #Create an empy df\n",
    "for restaurant in restaurant_list:\n",
    "    #Reviews in page 1\n",
    "    page = 0\n",
    "    geo = restaurant['geo']\n",
    "    locationId = restaurant['locationId']\n",
    "    restaurant_name = restaurant['restaurant_name']\n",
    "    location = restaurant['location']\n",
    "\n",
    "    print('Scraping ',restaurant_name, 'in ', location) #just to know what the scrapper is doing\n",
    "\n",
    "    link = f'https://www.tripadvisor.com/Restaurant_Review-{geo}-{locationId}-Reviews-{restaurant_name}-{location}'\n",
    "    get_data_return = get_data(link, driver, page, df)\n",
    "    df = get_data_return[0]\n",
    "    soup = get_data_return[1]\n",
    "    print(link,'done')\n",
    "    time.sleep(1)\n",
    "    if soup.find(class_= 'nav next ui_button primary') != None:\n",
    "\n",
    "#To proceed with reviews in page 2 to N, we define a while function (while the \"next\" o \"siguiente\" botton is not\n",
    "# disabled, then the scrapper needs to continue gathering reviews). Since the link changes for each page, we can\n",
    "# use that to scrappe each page.\n",
    "    # 2nd to N-1 pages\n",
    "        while soup.find(class_= 'nav next ui_button primary disabled') == None:\n",
    "                page+=1\n",
    "                link = f'https://www.tripadvisor.com/Restaurant_Review-{geo}-{locationId}-Reviews-or{page}0-{restaurant_name}-{location}'\n",
    "                get_data_return = get_data(link, driver, page, df)\n",
    "                df = get_data_return[0]\n",
    "                soup = get_data_return[1]\n",
    "                time.sleep(random.randint(2,3))\n",
    "                print(link,' done')\n",
    "\n",
    "        #Code for the last page of reviews\n",
    "        if soup.find(class_= 'nav next ui_button primary disabled') == None:\n",
    "                page+=1\n",
    "                link = f'https://www.tripadvisor.com/Restaurant_Review-{geo}-{locationId}-Reviews-or{page}0-{restaurant_name}-{location}'\n",
    "                get_data_return = get_data(link, driver, page, df)\n",
    "                df = get_data_return[0]\n",
    "                soup = get_data_return[1]\n",
    "                print(link,' done')\n",
    "\n",
    "    \n",
    "print('JOB DONE')\n",
    "df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"reviews\", mode=\"a\", index=False, header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBTAINING RESTAURANT INFORMATION (CHARACTERISTICS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to gather all the relevant information of each restaurant in a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data1(link, driver, page, df):\n",
    "\n",
    "      driver.get(link) #Gets in the first restaurant webpage\n",
    "      time.sleep(random.randint(1,2))\n",
    "\n",
    "      #---aqui agafo tots els requests que el driver fa al network pq es l'unica manera d'obtenir la location\n",
    "      logs = driver.get_log(\"performance\")\n",
    "\n",
    "      #In case page for the reviews = 0, then a popup might appear and we make the robot to close it\n",
    "      if page == '0':\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]').click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "      \n",
    "      soup = BeautifulSoup(driver.page_source, 'html.parser') #Create element with html parsed that allows us \n",
    "      #to extract data from it\n",
    "\n",
    "      #RESTAURANT NAME\n",
    "      local_name = soup.title.text.split(',')[0] #We don't need to put the name of the restaurant in the reviews loop\n",
    "      #because is always part of the same page. It's in the tab of the website (that's the title) \n",
    "            \n",
    "      #NUMBER OF REVIEWS\n",
    "      try: \n",
    "            nreviews = soup.find(class_ = 'AfQtZ').text.split(' ')[0]\n",
    "      except: nreviews = \"0\"\n",
    "\n",
    "      #AVERAGE GRADE\n",
    "      try:\n",
    "            avgrade = soup.find(class_ = \"ZDEqb\").text\n",
    "      except: avgrade = \"N/A\"\n",
    "\n",
    "      #PRICE RANGE\n",
    "      #try:\n",
    "            #priceRange = soup.find(class_ = \"dlMOJ\").text\n",
    "      #except: priceRange = \"N/A\"\n",
    "\n",
    "      #Loop details\n",
    "      #---posar variables a NA d'entrada aixi si no ho troba no em dona error a l'assignar el valor al diccionari \"dict_aux\"\n",
    "      #---he tret l'if perquè és el que causava l'error. Jo els borraria\n",
    "      price, cuisine_types, special_diets, meals, advantages, location = \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
    "      try:\n",
    "            try: \n",
    "                  \n",
    "                  for div in soup.find(class_ = \"BMlpu\"):\n",
    "                  #Get PRICE\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"PRICE RANGE\":\n",
    "                              price = div.find(class_ = \"SrqKb\").text\n",
    "                        #else: price = \"N/A\"\n",
    "                              \n",
    "                  #Get cuisine type\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"CUISINES\":\n",
    "                              cuisine_types = div.find(class_ = \"SrqKb\").text \n",
    "                        #else: cuisine_types = \"N/A\"\n",
    "                  \n",
    "                  #Get special diet\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"Special Diets\":\n",
    "                              special_diets = div.find(class_ = \"SrqKb\").text\n",
    "                        #else: special_diets = \"N/A\" \n",
    "\n",
    "                  #Get meals\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"Meals\":\n",
    "                              meals = div.find(class_ = \"SrqKb\").text\n",
    "                        #else: special_diets = \"N/A\" \n",
    "\n",
    "                  #Get advantages\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"Functions\":\n",
    "                              advantages = div.find(class_ = \"SrqKb\").text\n",
    "                        #else: special_diets = \"N/A\"\n",
    "      \n",
    "            except:\n",
    "                  \n",
    "                  \n",
    "                  for div in soup.find(class_ = \"RHcXN\"):\n",
    "                  #Get PRICE\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"PRICE RANGE\":\n",
    "                              price = div.find(class_ = \"SrqKb\").text\n",
    "                        #else: price = \"N/A\"\n",
    "                              \n",
    "                  #Get cuisine type\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"CUISINES\":\n",
    "                              cuisine_types = div.find(class_ = \"SrqKb\").text \n",
    "                        #else: cuisine_types = \"N/A\"\n",
    "                  \n",
    "                  #Get special diet\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"Special Diets\":\n",
    "                              special_diets = div.find(class_ = \"SrqKb\").text\n",
    "                        #else: special_diets = \"N/A\" \n",
    "\n",
    "                  #Get meals\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"Meals\":\n",
    "                              meals = div.find(class_ = \"SrqKb\").text\n",
    "                        #else: special_diets = \"N/A\" \n",
    "\n",
    "                  #Get advantages\n",
    "                        if div.find(class_ = \"tbUiL b\").text == \"Functions\":\n",
    "                              advantages = div.find(class_ = \"SrqKb\").text\n",
    "                        #else: special_diets = \"N/A\"\n",
    "      except: pass\n",
    "      #LOCATION\n",
    "      network_string = '.png|' #--- l'string aquest es el que em marca on després hi haura les coordenades\n",
    "      network_logs = [log for log in logs if (network_string in str(log))] #---list comprehension per filtrar nomes els requests que tinguin el network string dins el seu text\n",
    "\n",
    "      for network_log in network_logs: #---bucle per llegir els diferents logs que han passat el filtre\n",
    "            if len(location)>7: #--- aixo es pq si troba les coordenades surti del loop for pq no vull q el faci tot si ja te dades\n",
    "                  break\n",
    "            try:\n",
    "                  #---aqui basicament entro dins el diccionari, com que es passsa a string li faig un json.loads i entro a les diferents claus... \n",
    "                  #...per obtenir el valor que vull, amb splits i agafant el 1r valor de la llista que em queda\n",
    "                  location = json.loads(network_log['message'])['message']['params']['request']['url'].split('.png|')[1].split('&')[0]\n",
    "                  print(location)\n",
    "            except:\n",
    "                  location = \"loc N/A\"\n",
    "                  pass\n",
    "            \n",
    "      dict_aux = {\n",
    "            'restaurant': [local_name],\n",
    "            'nreviews': [nreviews],\n",
    "            'avgrade': [avgrade],\n",
    "            'price': [price],\n",
    "            'cuisine_types' : [cuisine_types],\n",
    "            'special_diets' : [special_diets],\n",
    "            'meals': [meals],\n",
    "            'advantages': [advantages],\n",
    "            #'priceRange': [priceRange],\n",
    "            'location': [location]}\n",
    "\n",
    "      df_aux = pd.DataFrame(dict_aux) #Names of the columns\n",
    "      df = pd.concat([df, df_aux], axis=0) #Giving names/labels to the columns of our dataframe\n",
    "\n",
    "      df.reset_index(drop=True, inplace=True) #To enumerate the dataframe rows\n",
    "\n",
    "      return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a loop with the function that gathers relevant information for each restaurant. We need the loop to access each restaurant using their link and the changing parts that we collected in the restaurants list part:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame() #Create an empy df\n",
    "\n",
    "for restaurant in restaurant_list:\n",
    "    page = 0\n",
    "    geo = restaurant['geo']\n",
    "    locationId = restaurant['locationId']\n",
    "    restaurant_name = restaurant['restaurant_name']\n",
    "    location = restaurant['location']\n",
    "\n",
    "    print('Scraping ',restaurant_name, 'in ', location) #just to know what the scrapper is doing\n",
    "\n",
    "    link = f'https://www.tripadvisor.com/Restaurant_Review-{geo}-{locationId}-Reviews-{restaurant_name}-{location}'\n",
    "    get_data_return = get_data1(link, driver, page, df)\n",
    "    df = get_data_return\n",
    "    #soup = get_data_return[1]\n",
    "    print(link,'done')\n",
    "    time.sleep(1)\n",
    "\n",
    "print('JOB DONE')\n",
    "df\n",
    "df.to_csv(\"restaurant_price_aarhus\", index = False, encoding = \"utf-8\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE THE DATAFRAME IN A CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"restaurant_features_aarhus\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preu rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data1(link, driver, page, df):\n",
    "\n",
    "      driver.get(link) #Gets in the first restaurant webpage\n",
    "      time.sleep(random.randint(1,2))\n",
    "\n",
    "      #---aqui agafo tots els requests que el driver fa al network pq es l'unica manera d'obtenir la location\n",
    "      logs = driver.get_log(\"performance\")\n",
    "\n",
    "      #In case page for the reviews = 0, then a popup might appear and we make the robot to close it\n",
    "      if page == '0':\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]').click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "      \n",
    "      soup = BeautifulSoup(driver.page_source, 'html.parser') #Create element with html parsed that allows us \n",
    "      #to extract data from it\n",
    "\n",
    "      #RESTAURANT NAME\n",
    "      local_name = soup.title.text.split(',')[0] #We don't need to put the name of the restaurant in the reviews loop\n",
    "      #because is always part of the same page. It's in the tab of the website (that's the title) \n",
    "\n",
    "      #PRICE RANGE\n",
    "      try:\n",
    "              priceRange = soup.find(class_ = \"dlMOJ\").text\n",
    "      except: priceRange = \"N/A\"\n",
    "\n",
    "      \n",
    "            \n",
    "      dict_aux = {\n",
    "            'restaurant': [local_name],\n",
    "            'priceRange': [priceRange]}\n",
    "\n",
    "      df_aux = pd.DataFrame(dict_aux) #Names of the columns\n",
    "      df = pd.concat([df, df_aux], axis=0) #Giving names/labels to the columns of our dataframe\n",
    "\n",
    "      df.reset_index(drop=True, inplace=True) #To enumerate the dataframe rows\n",
    "\n",
    "      return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab4e551504f799649c533f245c1eda83a6954a2baa7a4859287663554eaa87d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
